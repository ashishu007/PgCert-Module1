@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{sun2019fine,
  title={How to fine-tune BERT for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={China National Conference on Chinese Computational Linguistics},
  pages={194--206},
  year={2019},
  organization={Springer}
}

@INPROCEEDINGS{Sintoris2017bpm,
author={K. {Sintoris} and K. {Vergidis}},
booktitle={2017 IEEE 19th Conference on Business Informatics (CBI)},
title={Extracting Business Process Models Using Natural Language Processing (NLP) Techniques},
year={2017},
volume={01},
number={},
pages={135-139},
keywords={business data processing;natural language processing;organisational aspects;business process model extraction;natural language processing techniques;NLP techniques;Doctoral Consortium paper;syntactic sentence structure;grammatical sentence structure;BPMN;business process modeling technique;Business;Natural language processing;Tools;Prototypes;Syntactics;Algorithm design and analysis;Analytical models;Business Process;Natural Language Processing;Information Extraction;Business Process Modelling;Business Process Management Systems;Part-Of-Speech Tagging},
doi={10.1109/CBI.2017.41},
ISSN={2378-1971},
month={July},}

@inproceedings{Bordignon2018bpm,
author = {Bordignon, Ana and Thom, Lucinéia and Soares Silva, Thanner and Stein Dani, Vinicius and Fantinato, Marcelo and Ferreira, Renato},
year = {2018},
month = {06},
pages = {},
title = {Natural Language Processing in Business Process Identification and Modeling: A Systematic Literature Review},
doi = {10.1145/3229345.3229373}
}

@article{manning2015computational,
  title={Computational linguistics and deep learning},
  author={Manning, Christopher D},
  journal={Computational Linguistics},
  volume={41},
  number={4},
  pages={701--707},
  year={2015},
  publisher={MIT Press}
}

@misc{budek_2019, 
title={A business guide to Natural Language Processing (NLP)}, url={https://deepsense.ai/a-business-guide-to-natural-language-processing-nlp/}, journal={deepsense.ai}, publisher={Konrad Budek https://deepsense.ai/wp-content/uploads/2019/04/DS_logo_color.svg}, author={Budek, Konrad}, year={2019}, month={Sep}}


@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year={1993}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@article{weber2005textual,
  title={Textual case-based reasoning},
  author={Weber, Rosina O and Ashley, Kevin D and Br{\"u}ninghaus, Stefanie},
  journal={The Knowledge Engineering Review},
  volume={20},
  number={3},
  pages={255--260},
  year={2005},
  publisher={Cambridge University Press}
}


@article{aamodt1994case,
  title={Case-based reasoning: Foundational issues, methodological variations, and system approaches},
  author={Aamodt, Agnar and Plaza, Enric},
  journal={AI communications},
  volume={7},
  number={1},
  pages={39--59},
  year={1994},
  publisher={IOS press}
}

@inproceedings{lai-etal-2018-sunnynlp,
    title = "{SUNNYNLP} at {S}em{E}val-2018 Task 10: A Support-Vector-Machine-Based Method for Detecting Semantic Difference using Taxonomy and Word Embedding Features",
    author = "Lai, Sunny  and
      Leung, Kwong Sak  and
      Leung, Yee",
    booktitle = "Proceedings of The 12th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S18-1118",
    doi = "10.18653/v1/S18-1118",
    pages = "741--746",
    abstract = "We present SUNNYNLP, our system for solving SemEval 2018 Task 10: {``}Capturing Discriminative Attributes{''}. Our Support-Vector-Machine(SVM)-based system combines features extracted from pre-trained embeddings and statistical information from Is-A taxonomy to detect semantic difference of concepts pairs. Our system is demonstrated to be effective in detecting semantic difference and is ranked 1st in the competition in terms of F1 measure. The open source of our code is coined SUNNYNLP.",
}

@inproceedings{baldini-soares-etal-2019-matching,
    title = "Matching the Blanks: Distributional Similarity for Relation Learning",
    author = "Baldini Soares, Livio  and
      FitzGerald, Nicholas  and
      Ling, Jeffrey  and
      Kwiatkowski, Tom",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1279",
    doi = "10.18653/v1/P19-1279",
    pages = "2895--2905",
    abstract = "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris{'} distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task{'}s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",
}

@article{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{Bohnet_2018,
   title={Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings},
   url={http://dx.doi.org/10.18653/v1/P18-1246},
   DOI={10.18653/v1/p18-1246},
   journal={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Bohnet, Bernd and McDonald, Ryan and Simões, Gonçalo and Andor, Daniel and Pitler, Emily and Maynez, Joshua},
   year={2018}
}


@article{Heinzerling_2019,
   title={Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation},
   url={http://dx.doi.org/10.18653/v1/p19-1027},
   DOI={10.18653/v1/p19-1027},
   journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Heinzerling, Benjamin and Strube, Michael},
   year={2019}
}


@inproceedings{strakova-etal-2019-neural,
    title = "Neural Architectures for Nested {NER} through Linearization",
    author = "Strakov{\'a}, Jana  and
      Straka, Milan  and
      Hajic, Jan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1527",
    doi = "10.18653/v1/P19-1527",
    pages = "5326--5331",
    abstract = "We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.",
}

@article{baevski2019cloze,
  title={Cloze-driven pretraining of self-attention networks},
  author={Baevski, Alexei and Edunov, Sergey and Liu, Yinhan and Zettlemoyer, Luke and Auli, Michael},
  journal={arXiv preprint arXiv:1903.07785},
  year={2019}
}

@MISC{email_site,
author = {Tschabitscher, Heinz},
title = {19 Fascinating Email Facts},
month = {January},
year = {2020},
howpublished={\url{https://www.lifewire.com/how-many-emails-are-sent-every-day-1171210}}
}

@article{bandhakavi2017lexicon,
  title={Lexicon based feature extraction for emotion text classification},
  author={Bandhakavi, Anil and Wiratunga, Nirmalie and Padmanabhan, Deepak and Massie, Stewart},
  journal={Pattern recognition letters},
  volume={93},
  pages={133--142},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{bandhakavi2018context,
  title={Context Extraction for Aspect-Based Sentiment Analytics: Combining Syntactic, Lexical and Sentiment Knowledge},
  author={Bandhakavi, Anil and Wiratunga, Nirmalie and Massie, Stewart and Luhar, Rushi},
  booktitle={International Conference on Innovative Techniques and Applications of Artificial Intelligence},
  pages={357--371},
  year={2018},
  organization={Springer}
}

@inproceedings{mukras2007selecting,
  title={Selecting bi-tags for sentiment analysis of text},
  author={Mukras, Rahman and Wiratunga, Nirmalie and Lothian, Robert},
  booktitle={International Conference on Innovative Techniques and Applications of Artificial Intelligence},
  pages={181--194},
  year={2007},
  organization={Springer}
}


@article{kobayashi2018contextual,
  title={Contextual augmentation: Data augmentation by words with paradigmatic relations},
  author={Kobayashi, Sosuke},
  journal={arXiv preprint arXiv:1805.06201},
  year={2018}
}

@InProceedings{bojar-EtAl:2014:W14-33,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}


@inproceedings{riedel2010modeling,
  title={Modeling relations and their mentions without labeled text},
  author={Riedel, Sebastian and Yao, Limin and McCallum, Andrew},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={148--163},
  year={2010},
  organization={Springer}
}

@article{han2018fewrel,
  title={Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation},
  author={Han, Xu and Zhu, Hao and Yu, Pengfei and Wang, Ziyun and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:1810.10147},
  year={2018}
}

@inproceedings{krebs2018semeval,
  title={Semeval-2018 task 10: Capturing discriminative attributes},
  author={Krebs, Alicia and Lenci, Alessandro and Paperno, Denis},
  booktitle={Proceedings of The 12th International Workshop on Semantic Evaluation},
  pages={732--740},
  year={2018}
}


@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}


@inproceedings{silveira14gold,
  year = {2014},
  author = {Natalia Silveira and Timothy Dozat and Marie-Catherine de
	  Marneffe and Samuel Bowman and Miriam Connor and John Bauer and
	  Christopher D. Manning},
  title = {A Gold Standard Dependency Corpus for {E}nglish},
  booktitle = {Proceedings of the Ninth International Conference on Language
    Resources and Evaluation (LREC-2014)}
}

@article{sang2003introduction,
  title={Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition},
  author={Sang, Erik F and De Meulder, Fien},
  journal={arXiv preprint cs/0306050},
  year={2003}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{speer-lowry-duda-2018-luminoso,
    title = "{L}uminoso at {S}em{E}val-2018 Task 10: Distinguishing Attributes Using Text Corpora and Relational Knowledge",
    author = "Speer, Robyn  and
      Lowry-Duda, Joanna",
    booktitle = "Proceedings of The 12th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S18-1162",
    doi = "10.18653/v1/S18-1162",
    pages = "985--989",
    abstract = "Luminoso participated in the SemEval 2018 task on {``}Capturing Discriminative Attributes{''} with a system based on ConceptNet, an open knowledge graph focused on general knowledge. In this paper, we describe how we trained a linear classifier on a small number of semantically-informed features to achieve an F1 score of 0.7368 on the task, close to the task{'}s high score of 0.75.",
}

@misc{lan2019albert,
    title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
    year={2019},
    eprint={1909.11942},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{edunov2018understanding,
  title={Understanding back-translation at scale},
  author={Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
  journal={arXiv preprint arXiv:1808.09381},
  year={2018}
}


@inproceedings{he-etal-2018-jointly,
    title = "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
    author = "He, Luheng  and
      Lee, Kenton  and
      Levy, Omer  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2058",
    doi = "10.18653/v1/P18-2058",
    pages = "364--369",
    abstract = "Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.",
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{graves2013hybrid,
  title={Hybrid speech recognition with deep bidirectional LSTM},
  author={Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  booktitle={2013 IEEE workshop on automatic speech recognition and understanding},
  pages={273--278},
  year={2013},
  organization={IEEE}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@article{Yang2018,
abstract = {Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.},
archivePrefix = {arXiv},
arxivId = {1806.04822},
author = {Yang, Pengcheng and Sun, Xu and Li, Wei and Ma, Shuming and Wu, Wei and Wang, Houfeng},
eprint = {1806.04822},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(7).pdf:pdf},
title = {{SGM: Sequence Generation Model for Multi-label Classification}},
url = {http://arxiv.org/abs/1806.04822},
year = {2018}
}
@article{Sachan2019,
abstract = {In this paper, we study bidirectional LSTM network for the task of text classification using both supervised and semisupervised approaches. Several prior works have suggested that either complex pretraining schemes using unsupervised methods such as language modeling (Dai and Le 2015; Miyato, Dai, and Goodfellow 2016) or complicated models (Johnson and Zhang 2017) are necessary to achieve a high classification accuracy. However, we develop a training strategy that allows even a simple BiLSTM model, when trained with cross-entropy loss, to achieve competitive results compared with more complex approaches. Furthermore, in addition to cross-entropy loss, by using a combination of entropy minimization, adversarial, and virtual adversarial losses for both labeled and unlabeled data, we report state-of-theart results for text classification task on several benchmark datasets. In particular, on the ACL-IMDB sentiment analysis and AG-News topic classification datasets, our method outperforms current approaches by a substantial margin. We also show the generality of the mixed objective function by improving the performance on relation extraction task.1},
author = {Sachan, Devendra Singh and Zaheer, Manzil and Salakhutdinov, Ruslan},
doi = {10.1609/aaai.v33i01.33016940},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(9).pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {adversarial train-,semi-supervised learning,text classification},
pages = {6940--6948},
title = {{Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function}},
volume = {33},
year = {2019}
}
@article{Wang2010,
abstract = {Objective To investigate the appropriate criteria for the screening of retinopathy of prematurity ( ROP) in Xi'an region and to determine the risk factors for ROP. Methods Screening criteria were established basing on the ROP screening guidelines, implemented by Ministry of Public Health with minor modification. It included newborn infants with birth weight of 2000 g or less, or gestational age of 34 w or less. The results obtained by the present screening criteria were compared with those obtained from the national screening criteria Gestation age and birth weight between groups of ROP and normal fundus were compared by Student t - test, the ratios of persistent oxygen inhalation were examined by Chi-square test, and the risk factors for ROP were analyzed by Fisher exact test A P {\textless} 0.05 was considered significant Results In all the infants examined, 18 cases (36 eyes, 13.43{\%}) developed ROP, including 4 cases (8 eyes) suffering from stage 1, 5 cases (10 eyes) from stage 2 (one was affected in zone II with plus disease) , 5 cases (10 eyes) from stage 3 with plus disease, 1 case (2 eyes) from stage 4 A, 1 case (2 eyes) from stage 5, and 2 cases (4 eyes) from regressed stage. In ROP cases, gestation age ranged from 28 to 34 w, (30.58 ± 1.97) w; birth weight ranged from 880 to 1950 g, ( 1388.89 ±268.39) g. Statistical analysis showed that, the gestation age and birth weight in normal fundus group were (32.56 ±2.00) w and (1773.91 ±349.73) g respectively, which were higher than group of ROP (gestation age t =3. 90, P {\textless} 0.01 ; birth weight t =4.45, P {\textless}0.01 ). The ratios of persistent oxygen inhalation in group of normal fundus and ROP were 59.48{\%} and 61.11{\%} ( Χ2 = 0.017, P {\textgreater} 0.05 ) . It was also observed that the ratios of hypoxic - ischemic encephalopathy and placenta abruption in those two groups were 12. 07{\%} , 33. 33{\%} (P = 0.030) and 0.86{\%} , 11.11{\%} (P=0.047). Clinical analysis indicated that prematurity, low birth weight, hypoxic-ischemic encephalopathy and placenta abruption were closely related to the occurrence of ROP. On the other hand, if using national screening criteria, only 108 infants in all 134 cases required screening, and all of the ROP cases could be detected with a detection rate at 16.67{\%}. Conclusions The detection rate of ROP is 13.43{\%} (18/134) in the present study, which is consistent with the results obtained in previous studies in China The national criteria are appropriate for the ROP screening in Xi'an region. Prematurity, low birth weight and the relative hypoxia are high risk factors for the occurrence of ROP.},
author = {Wang, Yu Sheng and Zhang, Zi Feng and Li, Man Hong and Zhang, Peng and Liu, Xiao Yan},
doi = {10.3760/cma.j.issn.04124081.2010.02.006},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(8).pdf:pdf},
issn = {04124081},
journal = {Chinese Journal of Ophthalmology},
keywords = {Epidemiology,Mass screening,Retinopathy of prematurity},
number = {2},
pages = {119--124},
pmid = {20388344},
title = {{Preliminary results of screening of retinopathy of prematurity in Xi' an}},
volume = {46},
year = {2010}
}
@article{Liu2016,
abstract = {Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multitask learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.},
archivePrefix = {arXiv},
arxivId = {1605.05101},
author = {Liu, Pengfei and Qiu, Xipeng and Xuanjing, Huang},
eprint = {1605.05101},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(4).pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Natural Language Processing},
pages = {2873--2879},
title = {{Recurrent neural network for text classification with multi-task learning}},
volume = {2016-Janua},
year = {2016}
}
@article{Zhou2016,
abstract = {Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variable-length text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (ID) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying ID pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.},
archivePrefix = {arXiv},
arxivId = {1611.06639},
author = {Zhou, Peng and Qi, Zhenyu and Zheng, Suncong and Xu, Jiaming and Bao, Hongyun and Xu, Bo},
eprint = {1611.06639},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(6).pdf:pdf},
isbn = {9784879747020},
journal = {COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers},
number = {1},
pages = {3485--3495},
title = {{Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling}},
volume = {2},
year = {2016}
}
@article{Adhikari2019,
abstract = {Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.},
author = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
doi = {10.18653/v1/n19-1408},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(10).pdf:pdf},
pages = {4046--4051},
title = {{Rethinking Complex Neural Network Architectures for Document Classification}},
year = {2019}
}
@article{Chang2008,
abstract = {Traditionally, text categorization has been studied as the problem of training of a classifier using labeled data. However, people can categorize documents into named categories without any explicit training because we know the meaning of category names. In this paper, we introduce Dataless Classification, a learning protocol that uses world knowledge to induce classifiers without the need for any labeled data. Like humans, a dataless classifier interprets a string of words as a set of semantic concepts. We propose a model for dataless classification and show that the label name alone is often sufficient to induce classifiers. Using Wikipedia as our source of world knowledge, we get 85.29{\%} accuracy on tasks from the 20 Newsgroup dataset and 88.62{\%} accuracy on tasks from a Yahoo! Answers dataset without any labeled or unlabeled data from the datasets. With unlabeled data, we can further improve the results and show quite competitive performance to a supervised learning algorithm that uses 100 labeled examples. Copyright {\textcopyright} 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
author = {Chang, Ming Wei and Ratinov, Lev and Roth, Dan and Srikumar, Vivek},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(2).pdf:pdf},
isbn = {9781577353683},
journal = {Proceedings of the National Conference on Artificial Intelligence},
pages = {830--835},
title = {{Importance of semantic representation: Dataless classification}},
volume = {2},
year = {2008}
}
@inproceedings{Konoplich2018,
abstract = {Named Entity Recognition is one of the most popular tasks of the natural language processing. Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for natural language processing tasks. However, in most cases, a recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively few labeled data. Also, there are many difficulties in processing Russian language. In this paper, we present a semi-supervised approach for adding deep contextualized word representation that models both complex characteristics of word usage (e.g., syntax and semantics), and how these usages vary across linguistic contexts (i.e., to model polysemy). Here word vectors are learned functions of the internal states of a deep bidirectional language model, which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and be combined with other word representation features. We evaluate our model on FactRuEval-2016 dataset for named entity recognition in Russian and achieve state of the art results.},
author = {Konoplich, Georgy and Putin, Evgeniy and Filchenkov, Andrey and Rybka, Roman},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-030-01204-5_5},
isbn = {9783030012038},
issn = {18650929},
keywords = {Bi-LSTM,Language modeling,NER,Semi-supervised learning,Word representation},
title = {{Named entity recognition in Russian with word representation learned by a bidirectional language model}},
year = {2018}
}

@article{taylor1953cloze,
  title={“Cloze procedure”: A new tool for measuring readability},
  author={Taylor, Wilson L},
  journal={Journalism Bulletin},
  volume={30},
  number={4},
  pages={415--433},
  year={1953},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf},
  year={2018}
}

@techreport{Arendarenko2012a,
abstract = {We would like to introduce BEECON, an information and event extraction system for business intelligence. This is the first ontology-based system for business documents analysis that is able to detect 41 different types of business events from unstructured sources of information. The described system is intended to enhance business intelligence efficiency by automatically extracting relevant content such as business entities and events. In order to achieve it, we use natural language processing techniques, pattern recognition algorithms and handwritten detection rules. In our test set consisting of 190 documents with 550 events, the system achieved 95{\%} precision and 67{\%} recall in detecting all supported business event types from newspaper texts.},
author = {Arendarenko, Ernest and Kakkonen, Tuomo},
booktitle = {LNAI},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arendarenko, Kakkonen - 2012 - LNAI 7557 - Ontology-Based Information and Event Extraction for Business Intelligence.pdf:pdf},
keywords = {business intelligence,event extraction,ontology-based information extraction},
pages = {89--102},
title = {{LNAI 7557 - Ontology-Based Information and Event Extraction for Business Intelligence}},
volume = {7557},
year = {2012}
}
@article{Adeyanju,
author = {Adeyanju, Ibrahim and Craw, Susan and Ghose, Abhishek and Gray, Allyson and Wiratunga, Nirmalie},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adeyanju et al. - Unknown - ˆ An Arpeggio of Tastes ⋆ RaGo Ut.pdf:pdf},
number = {November 2016},
title = {{ˆ An Arpeggio of Tastes ⋆ RaGo Ut :}}
}
@article{Adeyanju2010,
abstract = {The need for automated text evaluation is common to several AI disciplines. In this work, we explore the use of Machine Translation (MT) evaluation metrics for Textual Case Based Reasoning (TCBR). MT and TCBR typically propose textual solutions and both rely on human reference texts for evaluation purposes. Current TCBR evaluation metrics such as precision and recall employ a single human reference but these metrics are misleading when semantically similar texts are expressed with different sets of keywords. MT metrics overcome this challenge with the use of multiple human references. Here, we explore the use of multiple references as opposed to a single reference applied to incident reports from the medical domain. These references are created introspectively from the original dataset using the CBR similarity assumption. Results indicate that TCBR systems evaluated with these new metrics are closer to human judgements. The generated text in TCBR is typically similar in length to the reference since it is a revised form of an actual solution to a similar problem, unlike MT where generated texts can sometimes be significantly shorter. We therefore discovered that some parameters in the MT evaluation measures are not useful for TCBR due to the intrinsic difference in the text generation process. {\textcopyright} 2010 Springer-Verlag.},
author = {Adeyanju, Ibrahim and Wiratunga, Nirmalie and Lothian, Robert and Craw, Susan},
doi = {10.1007/978-3-642-14274-1_4},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adeyanju et al. - 2010 - Applying machine translation evaluation techniques to textual CBR.pdf:pdf},
isbn = {3642142737},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {21--35},
title = {{Applying machine translation evaluation techniques to textual CBR}},
volume = {6176 LNAI},
year = {2010}
}
@article{Adeyanju2015,
abstract = {Several techniques have been used to generate weather forecast texts. In this paper, case based reasoning (CBR) is proposed for weather forecast text generation because similar weather conditions occur over time and should have similar forecast texts. CBR-METEO, a system for generating weather forecast texts was developed using a generic framework (jCOLIBRI) which provides modules for the standard components of the CBR architecture. The advantage in a CBR approach is that systems can be built in minimal time with far less human effort after initial consultation with experts. The approach depends heavily on the goodness of the retrieval and revision components of the CBR process. We evaluated CBRMETEO with NIST, an automated metric which has been shown to correlate well with human judgements for this domain. The system shows comparable performance with other NLG systems that perform the same task.},
archivePrefix = {arXiv},
arxivId = {1509.01023},
author = {Adeyanju, Ibrahim},
eprint = {1509.01023},
file = {:C$\backslash$:/Users/User/Documents/Papers/TCBR/weather forecast rgu.pdf:pdf},
keywords = {cbr,nlg,text generation,text reuse,weather forecast},
number = {10},
pages = {35--40},
title = {{Generating Weather Forecast Texts with Case Based Reasoning}},
url = {http://arxiv.org/abs/1509.01023},
volume = {45},
year = {2015}
}
@article{Agnar1994,
abstract = {Case-based reasoning is a recent approach to problem solving and learning that has got a lot of attention over the last few years. Originating in the US, the basic idea and underlying theories have spread to other continents, and we are now within a period of highly active research in case-based reasoning in Europe as well. This paper gives an overview of the foundational issues related to case-based reasoning, describes some of the leading methodological approaches within the field, and exemplifies the current state through pointers to some systems. Initially, a general framework is defined, to which the subsequent descriptions and discussions will refer. The framework is influenced by recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval, reuse, solution testing, and learning are summarized, and their actual realization is discussed in the light of a few example systems that represent different CBR approaches. We also discuss the role of case-based methods as one type of reasoning and learning method within an integrated system architecture. {\textcopyright} 1994 IOS Press and the authors.},
annote = {Introduction to CBR

Reveiw Article},
author = {Agnar, Aamodt and Plaza, Enric},
doi = {10.3233/AIC-1994-7104},
file = {:C$\backslash$:/Users/User/Documents/Papers/TCBR/cbr base.pdf:pdf},
issn = {09217126},
journal = {AI Communications},
number = {1},
pages = {39--59},
title = {{Case-Based reasoning: Foundational issues, methodological variations, and system approaches}},
volume = {7},
year = {1994}
}
@article{Adeyanju2009,
abstract = {This paper proposes textual reuse as the identification of reusable textual constructs in a retrieved solution text. This is done by annotating a solution text so that reusable sections are identifiable from those that need revision. We present a novel and generic architecture, Case Retrieval Reuse Net (CR2N), that can be used to generate these annotations to denote text content as reusable or not. Obtaining evidence for and against reuse is crucial for annotation accuracy, therefore a comparative evaluation of different evidence gathering techniques is presented. Evaluation on two domains of weather forecast revision and health {\&} safety incident reporting shows significantly better accuracy over a retrieve-only system and a comparable reuse technique. This also provides useful insight into the text revision stage. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Adeyanju, Ibrahim and Wiratunga, Nirmalie and Lothian, Robert and Sripada, Somayajulu and Lamontagne, Luc},
doi = {10.1007/978-3-642-02998-1_3},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adeyanju et al. - 2009 - Case retrieval reuse net (CR2N) An architecture for reuse of textual solutions.pdf:pdf},
isbn = {3642029973},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {14--28},
title = {{Case retrieval reuse net (CR2N): An architecture for reuse of textual solutions}},
volume = {5650 LNAI},
year = {2009}
}
@article{Dubey2018,
abstract = {We propose a Case-Based Reasoning(CBR) approach for content selection, which is an intermediate step towards generating textual summaries of time series data in the weather prediction domain. Specifically, we handle two significant challenges, the first involving multivariate data that warrants modeling of the interaction of two 'channels' (wind speed and direction in our context) and the second involving the effective integration of domain-specific knowledge in the form of rules with data from a case library of past instances of content selection. We present an approach that uses domain knowledge to transform a given raw time series instance into a representation that facilitates effective retrieval of relevant cases, which are then used for change point prediction. We empirically demonstrate that our approach combining CBR and domain rules outperforms classical content selection mechanisms that are based on rules or heuristics alone as well as those that are purely data-driven.},
author = {Dubey, Neha and Chakraborti, Sutanu and Khemani, Deepak},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dubey, Chakraborti, Khemani - 2018 - Content selection for time series summarization using case-based reasoning.pdf:pdf},
isbn = {9781577357964},
journal = {Proceedings of the 31st International Florida Artificial Intelligence Research Society Conference, FLAIRS 2018},
keywords = {Case-Based Reasoning},
pages = {395--398},
title = {{Content selection for time series summarization using case-based reasoning}},
year = {2018}
}
@article{Raghunandan2008,
abstract = {Textual-case based reasoning (TCBR) systems where the problem and solution are in free text form are hard to evaluate. In the absence of class information, domain experts are needed to evaluate solution quality, and provide relevance information. This approach is costly and time consuming. We propose three measures that can be used to compare alternate TCBR system configurations, in the absence of class information. The main idea is to quantify alignment as the degree to which similar problems have similar solutions. Two local measures capture this information by analysing similarity between problem and solution neighbourhoods at different levels of granularity, whilst a global measure achieves the same by analyzing similarity between problem and solution clusters. We determine the suitability of the proposed measures by studying their correlation with classifier accuracy on a health and safety incident reporting task. Strong correlation is observed with all three approaches with local measures being slightly superior over the global one. {\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
author = {Raghunandan, M. A. and Wiratunga, Nirmalie and Chakraborti, Sutanu and Massie, Stewart and Khemani, Deepak},
doi = {10.1007/978-3-540-85502-6_30},
file = {:C$\backslash$:/Users/User/Documents/Papers/TCBR/Evaluation{\_}Measures{\_}for{\_}TCBR{\_}Systems.pdf:pdf},
isbn = {3540855017},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {May 2014},
pages = {444--458},
title = {{Evaluation measures for TCBR systems}},
volume = {5239 LNAI},
year = {2008}
}
@article{Patterson2005,
abstract = {In this paper we present a novel methodology for textual case-based reasoning. This technique is unique in that it automatically discovers case and similarity knowledge, is language independent, is scaleable and facilitates semantic similarity between cases to be carried out inherently without the need for domain knowledge. In addition it provides an insight into the thematical content of the case-base as a whole, which enables users to better structure queries. We present an analysis of the competency of the system by assessing the quality of the similarity knowledge discovered and show how it is ideally suited to case-based retrieval (querying by example).},
author = {Patterson, David and Rooney, Niall and Dobrynin, Vladimir and Galushka, Mykola},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patterson et al. - 2005 - Sophia A novel approach for textual case-based reasoning.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {15--20},
title = {{Sophia: A novel approach for textual case-based reasoning}},
year = {2005}
}
@article{Dubey2018a,
author = {Dubey, Neha and Chakraborti, Sutanu and Khemani, Deepak},
file = {:C$\backslash$:/Users/User/Documents/Papers/TCBR/TextualSummarizationTimeSeries.pdf:pdf},
keywords = {cbr,natural language generation,time series},
title = {{Textual Summarization of Time Series using Case-Based Reasoning MASTER OF SCIENCE}},
year = {2018}
}
@article{Liu2003,
abstract = {Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, we first give empirical evidence that feature selection methods can improve the efficiency and performance of text clustering algorithm. Then we propose a new feature selection method called "Term Contribution (TC)" and perform a comparative study on a variety of feature selection methods for text clustering, including Document Frequency (DF), Term Strength (TS), Entropy-based (En), Information Gain (IG) and א2 statistic (CHI). Finally, we propose an "Iterative Feature Selection (IF)" method that addresses the unavailability of label problem by utilizing effective supervised feature selection method to iteratively select features and perform clustering. Detailed experimental results on Web Directory data are provided in the paper.},
author = {Liu, Tao and Liu, Shengping and Chen, Zheng and Ma, Wei Ying},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2003 - An Evaluation on Feature Selection for Text Clustering.pdf:pdf},
isbn = {1577351894},
journal = {Proceedings, Twentieth International Conference on Machine Learning},
pages = {488--495},
title = {{An Evaluation on Feature Selection for Text Clustering}},
volume = {2},
year = {2003}
}
@article{Sofie2007,
author = {Sofie, Ida and Stenerud, Gebhardt},
file = {:C$\backslash$:/Users/User/Documents/Papers/TCBR/ntnu thesis.pdf:pdf},
number = {June},
title = {{Case-based Reasoning in Text Document Classification}},
year = {2007}
}
@article{Zontangos2004,
abstract = {Networks play an important role in entrepreneurship but their role in growth is less well understood. Consequently we explore growth as a social, but strategic, practice. We thus consider the social nature of growth and the role of networking for growth. Employing two longitudinal cases we show how social interaction opportunities were enacted and growth enabled. We note how the networking practices involved specific patterns of activity, i.e. spans. We theorise these practices employing Bourdieus habitus. Our contribution is twofold. Theoretically, we offer a new conceptualisation of networking practices. Practically, we show how entrepreneurial growth takes place through collaborative practice.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0412138v1},
author = {Zontangos, G and Anderson, A.R},
doi = {10.1177/0266242610391936},
eprint = {0412138v1},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zontangos, Anderson - 2004 - OpenAIR @ RGU The Open Access Institutional Repository at Robert Gordon University.pdf:pdf},
isbn = {190108583X},
issn = {0166-4972},
journal = {Qualitative Market Research:An International Journal},
number = {3},
pages = {228--236},
pmid = {851953633},
primaryClass = {cond-mat},
title = {{OpenAIR @ RGU The Open Access Institutional Repository at Robert Gordon University}},
url = {http://eprints.qut.edu.au/29653/},
volume = {7},
year = {2004}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
annote = {Transformers Introduction PaperIntroduced a new deep learning architecture for machine translation.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/attention is all you need.pdf:pdf},
month = {jun},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@techreport{Nogueira,
abstract = {Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from character-to sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Tree-bank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7{\%} accuracy, and fine-grained classification, with 48.3{\%} accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4{\%}.},
author = {Nogueira, C{\'{i}}cero and Santos, Dos and Gatti, Ma{\'{i}}ra},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/deep cnn for sentiment analysis of short text.pdf:pdf},
pages = {69--78},
title = {{Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts}}
}
@misc{Cambria2014,
abstract = {Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of 'jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curves- which will eventually lead NLP research to evolve into natural language understanding.},
author = {Cambria, Erik and White, Bebo},
booktitle = {IEEE Computational Intelligence Magazine},
doi = {10.1109/MCI.2014.2307227},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/nlp-research-curve-com-intlg-ieee.pdf:pdf},
issn = {1556603X},
number = {2},
pages = {48--57},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Jumping NLP curves: A review of natural language processing research}},
volume = {9},
year = {2014}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/word2vec.pdf:pdf},
month = {jan},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/glove.pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.}
}
@book{Feldman2006,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
author = {Feldman, Ronen and Sanger, James},
booktitle = {The Text Mining Handbook},
doi = {10.1017/cbo9780511546914},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feldman, Sanger - 2006 - The Text Mining Handbook.pdf:pdf},
isbn = {9780521836579},
title = {{The Text Mining Handbook}},
year = {2006}
}
@book{Golemandaniel;boyatzisRichard;Mckee2019,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{goleman, daniel; boyatzis, Richard; Mckee}, Annie},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/goleman, daniel boyatzis, Richard Mckee - 2019 - 済無No Title No Title.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{済無No Title No Title}},
volume = {53},
year = {2019}
}
@article{Nkisi-orji2018,
author = {Nkisi-orji, Ikechukwu and Massie, Stewart},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nkisi-orji, Massie - 2018 - AZOTH Task Completion Report Task Conduct a literature review to identify and assess existing approaches (2).pdf:pdf},
title = {{AZOTH Task Completion Report Task : Conduct a literature review to identify and assess existing approaches to identifying relevant text snippets RGU ComplyAnts Title : Description : Lead : Support : Author : Date : Reviewed : Date : Current Version : Circ}},
year = {2018}
}
@book{Wosinska2007,
abstract = {The measurement of the two-particle correlation function for different particle species allows to obtain information about the development of the particle emission process: the space-time properties of emitting sources and the emission time sequence of different particles. The single-particle characteristics and two-particle correlation functions for neutral and charged particles registered in forward direction are used to determine that the heavy fragments (deuterons and tritons) are emitted in the first stage of the reaction (pre-equilibrium source) while the majority of neutrons and protons originates from the long-lived quasi-projectile. The emission time sequence of protons, neutrons and deuterons has been obtained from the analysis of non-identical particle correlation functions. {\textcopyright} Societ{\`{a}} Italiana di Fisica and Springer-Verlag 2007.},
author = {Wosi{\'{n}}ska, K. and Pluta, J. and Hanappe, F. and Stuttge, L. and Angelique, J. C. and Benoit, B. and {De Goes Brennand}, E. and Bizard, G. and Colin, J. and Costa, G. and Desesquelles, P. and Dorvaux, O. and Durand, D. and Erazmus, B. and Kuleshov, S. and Lednicky, R. and Marques, M. and Materna, Th and Mikhailov, K. and Papatheofanous, G. and Pawlak, T. and Staranowicz, A. and Stavinskiy, A. and Tamain, B. and Vlasov, A. and Vorobyev, L.},
booktitle = {European Physical Journal A},
doi = {10.1140/epja/i2006-10279-1},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wosi{\'{n}}ska et al. - 2007 - Correlations of neutral and charged particles in 40 Ar- 58 Ni reaction at 77 MeVu(2).pdf:pdf},
isbn = {9781484223871},
issn = {14346001},
number = {1},
pages = {55--59},
title = {{Correlations of neutral and charged particles in 40 Ar- 58 Ni reaction at 77 MeV/u}},
volume = {32},
year = {2007}
}
@article{Zhang2019,
abstract = {The explosive growth of biomedical literature has created a rich source of knowledge, such as that on protein-protein interactions (PPIs) and drug-drug interactions (DDIs), locked in unstructured free text. Biomedical relation classification aims to automatically detect and classify biomedical relations, which has great benefits for various biomedical research and applications. In the past decade, significant progress has been made in biomedical relation classification. With the advance of neural network methodology, neural network-based approaches have been applied in biomedical relation classification and achieved state-of-the-art performance for some public datasets and shared tasks. In this review, we describe the recent advancement of neural network-based approaches for classifying biomedical relations. We summarize the available corpora and introduce evaluation metrics. We present the general framework for neural network-based approaches in biomedical relation extraction and pretrained word embedding resources. We discuss neural network-based approaches, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We conclude by describing the remaining challenges and outlining future directions.},
author = {Zhang, Yijia and Lin, Hongfei and Yang, Zhihao and Wang, Jian and Sun, Yuanyuan and Xu, Bo and Zhao, Zhehuan},
doi = {10.1016/j.jbi.2019.103294},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Neural network-based approaches for biomedical relation classification A review.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {biomedical relation classification},
number = {June},
pages = {103294},
title = {{Neural network-based approaches for biomedical relation classification: A review}},
volume = {99},
year = {2019}
}
@article{Adeyanju2010a,
abstract = {Textual reuse is an integral part of textual case-based reasoning (TCBR) which deals with solving new problems by reusing previous similar problem-solving experiences documented as text. We investigate the role of text reuse for text authoring applications that involve feedback or review generation. Generally providing feedback in the form of assigning a rating from a likert scale is far easier compared to articulating explanatory feedback as text. When previous feedback generated about the same or similar objects are maintained as cases, there is opportunity for knowledge reuse. In this paper, we show how compositional and transformational adaptation techniques can be applied once sentences in a given case are aligned to relevant structured attribute values. Three text reuse algorithms are introduced and evaluated on a dataset gathered from online Hotel reviews from TripAdvisor. Here cases consists of both structured sub-rating attributes together with textual feedback. Generally, aligned sentences linked to similar sub-rating values are clustered together and prototypical sentences are then extracted to enable reuse across similar authors. Experiments show a close similarity between our proposed texts and actual human edited review text. We also found that problems with variability in vocabulary are best addressed when prototypes are formulated from larger sets of similar sentences in contrast to smaller sets from local neighbourhoods. {\textcopyright} 2010 The authors and IOS Press. All rights reserved.},
author = {Adeyanju, Ibrahim and Wiratunga, Nirmalie and Recio-Garc{\'{i}}a, Juan A. and Lothian, Robert},
doi = {10.3233/978-1-60750-606-5-777},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adeyanju et al. - 2010 - Learning to author text with textual CBR.pdf:pdf},
isbn = {9781607506058},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
pages = {777--782},
title = {{Learning to author text with textual CBR}},
volume = {215},
year = {2010}
}

@inproceedings{guo2018long,
  title={Long text generation via adversarial training with leaked information},
  author={Guo, Jiaxian and Lu, Sidi and Cai, Han and Zhang, Weinan and Yu, Yong and Wang, Jun},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@book{Espenakk2019,
author = {Espenakk, Erik and Knalstad, Magnus Johan and Kofod-Petersen, Anders},
doi = {10.1007/978-3-030-29249-2_5},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Espenakk, Knalstad, Kofod-Petersen - 2019 - Lazy Learned Screening for Efficient Recruitment(2).pdf:pdf},
isbn = {9783030292485},
issn = {16113349},
keywords = {Candidate ranking,Human resources,Recruitment,candidate ranking,human resources,recruitment},
pages = {64--78},
publisher = {Springer International Publishing},
title = {{Lazy Learned Screening for Efficient Recruitment}},
url = {http://dx.doi.org/10.1007/978-3-030-29249-2{\_}5},
volume = {2},
year = {2019}
}
@article{Hogenboom2016,
abstract = {Event extraction, a specialized stream of information extraction rooted back into the 1980s, has greatly gained in popularity due to the advent of big data and the developments in the related fields of text mining and natural language processing. However, up to this date, an overview of this particular field remains elusive. Therefore, we give a summarization of event extraction techniques for textual data, distinguishing between data-driven, knowledge-driven, and hybrid methods, and present a qualitative evaluation of these. Moreover, we discuss common decision support applications of event extraction from text corpora. Last, we elaborate on the evaluation of event extraction systems and identify current research issues.},
author = {Hogenboom, Frederik and Frasincar, Flavius and Kaymak, Uzay and {De Jong}, Franciska and Caron, Emiel},
doi = {10.1016/j.dss.2016.02.006},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hogenboom et al. - 2016 - A Survey of event extraction methods from text for decision support systems.pdf:pdf},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Event extraction,Information extraction,Natural language processing (NLP),Text mining},
pages = {12--22},
publisher = {Elsevier B.V.},
title = {{A Survey of event extraction methods from text for decision support systems}},
volume = {85},
year = {2016}
}
@article{Senses2019,
author = {Senses, Word},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Senses - 2019 - Word Senses and WordNet.pdf:pdf},
title = {{Word Senses and WordNet}},
year = {2019}
}
@article{Ramirez-Gallego2017,
abstract = {Data preprocessing and reduction have become essential techniques in current knowledge discovery scenarios, dominated by increasingly large datasets. These methods aim at reducing the complexity inherent to real-world datasets, so that they can be easily processed by current data mining solutions. Advantages of such approaches include, among others, a faster and more precise learning process, and more understandable structure of raw data. However, in the context of data preprocessing techniques for data streams have a long road ahead of them, despite online learning is growing in importance thanks to the development of Internet and technologies for massive data collection. Throughout this survey, we summarize, categorize and analyze those contributions on data preprocessing that cope with streaming data. This work also takes into account the existing relationships between the different families of methods (feature and instance selection, and discretization). To enrich our study, we conduct thorough experiments using the most relevant contributions and present an analysis of their predictive performance, reduction rates, computational time, and memory usage. Finally, we offer general advices about existing data stream preprocessing algorithms, as well as discuss emerging future challenges to be faced in the domain of data stream preprocessing.},
author = {Ram{\'{i}}rez-Gallego, Sergio and Krawczyk, Bartosz and Garc{\'{i}}a, Salvador and Wo{\'{z}}niak, Micha{\l} and Herrera, Francisco},
doi = {10.1016/j.neucom.2017.01.078},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ram{\'{i}}rez-Gallego et al. - 2017 - A survey on data preprocessing for data stream mining Current status and future directions.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Concept drift,Data discretization,Data mining,Data preprocessing,Data reduction,Data stream,Feature selection,Instance selection,Online learning},
pages = {39--57},
title = {{A survey on data preprocessing for data stream mining: Current status and future directions}},
volume = {239},
year = {2017}
}
@article{Bhatt2017,
abstract = {Internet of Things (IoT) has become a pervasive and diverse concept in recent years. IoT applications and services have given rise to a number of sub-fields in the IoT space. Wearable technology, with its particular set of characteristics and application domains, has formed a rapidly growing sub-field of IoT, viz., Wearable Internet of Things (WIoT). While numerous wearable devices are available in the market today, security and privacy are key factors for wide adoption of WIoT. Wearable devices are resource constrained by nature with limited storage, power, and computation. A Cloud-Enabled IoT (CEIoT) architecture, a dominant paradigm currently shaping the industry and suggested by many researchers, needs to be adopted for WIoT. In this paper, we develop an access control framework for cloud-enabled WIoT (CEWIoT) based on the Access Control Oriented (ACO) architecture recently developed for CEIoT in general. We first enhance the ACO architecture from the perspective of WIoT by adding an Object Abstraction Layer, and then develop our framework based on interactions between different layers of this enhanced ACO architecture. We present a general classification and taxonomy of IoT devices, along with brief introduction to various application domains of IoT and WIoT. We then present a remote health and fitness monitoring use case to illustrate different access control aspects of our framework and outline its possible enforcement in a commercial CEIoT platform, viz., AWS IoT. Finally, we discuss the objectives of our access control framework and relevant open problems.},
author = {Bhatt, Smriti and Patwa, Farhan and Sandhu, Ravi},
doi = {10.1109/CIC.2017.00050},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatt, Patwa, Sandhu - 2017 - An Access Control Framework for Cloud-Enabled Wearable Internet of Things.pdf:pdf},
isbn = {9781538625651},
journal = {Proceedings - 2017 IEEE 3rd International Conference on Collaboration and Internet Computing, CIC 2017},
keywords = {Access Control,Cloud Enabled,Internet of Things,IoT Devices,WIoT,Wearable Devices},
pages = {328--338},
title = {{An Access Control Framework for Cloud-Enabled Wearable Internet of Things}},
volume = {2017-Janua},
year = {2017}
}
@article{Kejriwal2019,
abstract = {Extracting information from both Web and natural language documents is the central step in knowledge graph construction, since it is the first line of attack in going from a corpus that is not machine-understandable or queryable to a semi-structured corpus that can be queried and reasoned over. Wrapper induction techniques were developed early in the Web community to deal with the special problem of extracting information from webpages and web templates. However, wrapper induction is not enough. Many key attributes need to be extracted directly from text using information extraction algorithms developed in the natural language processing community. This is also true in cases where the raw data is not from the Web, but is a corpus of natural language documents to begin with. Therefore, we also cover some established research on information extraction, including named entity recognition, relation extraction and event extraction. While the first of these has been around for quite some time, the last is a relatively novel research area where improving quality continues to be a challenge.},
author = {Kejriwal, Mayank},
doi = {10.1007/978-3-030-12375-8_2},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kejriwal - 2019 - Information Extraction.pdf:pdf},
isbn = {9780191743573},
issn = {21915776},
journal = {SpringerBriefs in Computer Science},
pages = {9--31},
title = {{Information Extraction}},
year = {2019}
}
@article{Wang2018,
abstract = {Background With the rapid adoption of electronic health records (EHRs), it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research. One critical component used to facilitate the secondary use of EHR data is the information extraction (IE) task, which automatically extracts and encodes clinical information from text. Objectives In this literature review, we present a review of recent published research on clinical information extraction (IE) applications. Methods A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In-Process {\&} Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and ACM Digital Library. Results A total of 1917 publications were identified for title and abstract screening. Of these publications, 263 articles were selected and discussed in this review in terms of publication venues and data sources, clinical IE tools, methods, and applications in the areas of disease- and drug-related studies, and clinical workflow optimizations. Conclusions Clinical IE has been used for a wide range of applications, however, there is a considerable gap between clinical studies using EHR data and studies using clinical IE. This study enabled us to gain a more concrete understanding of the gap and to provide potential solutions to bridge this gap.},
author = {Wang, Yanshan and Wang, Liwei and Rastegar-Mojarad, Majid and Moon, Sungrim and Shen, Feichen and Afzal, Naveed and Liu, Sijia and Zeng, Yuqun and Mehrabi, Saeed and Sohn, Sunghwan and Liu, Hongfang},
doi = {10.1016/j.jbi.2017.11.011},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2018 - Clinical information extraction applications A literature review.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Application,Clinical notes,Electronic health records,Information extraction,Natural language processing},
number = {November 2017},
pages = {34--49},
publisher = {Elsevier},
title = {{Clinical information extraction applications: A literature review}},
url = {https://doi.org/10.1016/j.jbi.2017.11.011},
volume = {77},
year = {2018}
}
@article{Arendarenko2012,
abstract = {We would like to introduce BEECON, an information and event extraction system for business intelligence. This is the first ontology-based system for business documents analysis that is able to detect 41 different types of business events from unstructured sources of information. The described system is intended to enhance business intelligence efficiency by automatically extracting relevant content such as business entities and events. In order to achieve it, we use natural language processing techniques, pattern recognition algorithms and hand-written detection rules. In our test set consisting of 190 documents with 550 events, the system achieved 95{\%} precision and 67{\%} recall in detecting all supported business event types from newspaper texts. 1 Introduction Business intelligence (BI) refers to techniques and tools that aim to provide businesses with support for decision-making by collecting and analyzing information relevant to business leaders. The enormous volume of textual information that is available in online sources and in the internal storage of organizations makes it nearly impossible to follow and analyze all the relevant sources manually or to apply traditional storage and retrieval methods. Useful business information is hidden in a mass of data that is constantly increasing and evolving. Information extraction (IE) deals with extracting relevant and high-quality information from unstructured sources, such as texts and document collections. The extraction process involves representing information contained in textual data in a structured and normalized way. BI offers an interesting field for applying IE techniques. The relevant information in the business context is represented by named entities (NE) and the relationships between them. We use the term IE to refer also to the extraction of events that are relevant to the entities that are being tracked. The purpose of the Business Events Extractor Component, based on the ONtology (BEECON) system introduced in this paper, is the automatic extraction of business-related entities (in particular company and product information) and events, by applying text processing methods. BEECON is based on an approach we refer to as},
author = {Arendarenko, Ernest and Kakkonen, Tuomo},
doi = {10.1007/978-3-642-33185-5_10},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arendarenko, Kakkonen - 2012 - LNAI 7557 - Ontology-Based Information and Event Extraction for Business Intelligence.pdf:pdf},
isbn = {978-3-642-33185-5},
journal = {Lnai},
keywords = {business intelligence,event extraction,ontology-based information extraction},
pages = {89--102},
title = {{Ontology-Based Information and Event Extraction for Business Intelligence}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-3-642-33185-5{\_}10.pdf},
volume = {7557},
year = {2012}
}
@article{Peng2017,
abstract = {Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.},
archivePrefix = {arXiv},
arxivId = {1708.03743},
author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
eprint = {1708.03743},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - 2017 - Cross-Sentence N-ary Relation Extraction with Graph LSTMs.pdf:pdf},
issn = {2307-387X},
title = {{Cross-Sentence N-ary Relation Extraction with Graph LSTMs}},
url = {http://arxiv.org/abs/1708.03743},
year = {2017}
}
@article{Upadhyay2018,
author = {Upadhyay, Ashish and Shrimali, Kushashwa Ravi and Shukla, Anupam},
doi = {10.1016/j.procs.2018.07.052},
issn = {18770509},
journal = {Procedia Computer Science},
pages = {424--431},
title = {{UAV-Robot Relationship for Coordination of Robots on a Collision Free Path}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918309979},
volume = {133},
year = {2018}
}
@article{Tripathi2018,
abstract = {{\textcopyright} 2018, Springer Science+Business Media, LLC, part of Springer Nature. Spectrum scarcity is one of the major challenges that the modern communication engineers are going through because of inefficient utilization of allocated frequency spectrum. The spectrum scarcity is a problem because there is not enough wavelengths/frequency to match the number of channels which are required to broadcast in a given bandwidth. Therefore, the utilization of available allocated spectrum when licensed users are not in use offers an opportunity as well as challenge, also, to increase the efficiency of spectrum utilization. Cognitive Radio offers a promising solution by reutilisation of unused allocated frequency spectrum. It helps to fulfil the demand of frequency requirement for modern communication system to accommodate more data transmission. In this optimum utilization of reuse of frequency spectrum required optimising algorithms in all parts of Cognitive Cycle. This paper focuses on designing a system based on fuzzy logic with a set of input and output parameters to obtain an optimised solution. A comparative analysis is also carried out among various types of membership functions of input and output on Mamdani Fuzzy Inference System and Sugeno Fuzzy Inference System. The proposed approach is applicable to design a better system model for a given set of rules.},
author = {Tripathi, S. and Upadhyay, A. and Kotyan, S. and Yadav, S.},
doi = {10.1007/s11277-018-6075-9},
issn = {1572834X},
journal = {Wireless Personal Communications},
keywords = {Cognitive radio,Fuzzy inference system,Mamdani,Spectrum scarcity,Sugeno},
title = {{Analysis and Comparison of Different Fuzzy Inference Systems Used in Decision Making for Secondary Users in Cognitive Radio Network}},
year = {2018}
}
@article{Young2018,
abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
archivePrefix = {arXiv},
arxivId = {1708.02709},
author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
doi = {10.1109/MCI.2018.2840738},
eprint = {1708.02709},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/dl trends in nlp.pdf:pdf},
issn = {15566048},
journal = {IEEE Computational Intelligence Magazine},
keywords = {and engineering,ek laboratories,nanyang technological university,school of computer science,singapore},
number = {3},
pages = {55--75},
title = {{Recent trends in deep learning based natural language processing [Review Article]}},
volume = {13},
year = {2018}
}
@article{Smith2019,
abstract = {This introduction aims to tell the story of how we put words into computers. It is part of the story of the field of natural language processing (NLP), a branch of artificial intelligence. It targets a wide audience with a basic understanding of computer programming, but avoids a detailed mathematical treatment, and it does not present any algorithms. It also does not focus on any particular application of NLP such as translation, question answering, or information extraction. The ideas presented here were developed by many researchers over many decades, so the citations are not exhaustive but rather direct the reader to a handful of papers that are, in the author's view, seminal. After reading this document, you should have a general understanding of word vectors (also known as word embeddings): why they exist, what problems they solve, where they come from, how they have changed over time, and what some of the open questions about them are. Readers already familiar with word vectors are advised to skip to Section 5 for the discussion of the most recent advance, contextual word vectors.},
archivePrefix = {arXiv},
arxivId = {1902.06006},
author = {Smith, Noah A.},
eprint = {1902.06006},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/contextual vectors review.pdf:pdf},
number = {February},
pages = {1--10},
title = {{Contextual Word Representations: A Contextual Introduction}},
url = {http://arxiv.org/abs/1902.06006},
year = {2019}
}
@article{Zhang2015,
abstract = {This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Zhang, Xiang and LeCun, Yann},
eprint = {1502.01710},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(5).pdf:pdf},
isbn = {0123456789},
pages = {1--9},
title = {{Text Understanding from Scratch}},
url = {http://arxiv.org/abs/1502.01710},
year = {2015}
}
@article{Vieira2017,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This paper show a series of experiments with Convolutional Neural Networks for sentence-level classification tasks with different hyperparameter settings and how sensitive model performance is to changes in these configurations.},
author = {Vieira, Jo{\~{a}}o Paulo Albuquerque and Moura, Raimundo Santos},
doi = {10.1109/CLEI.2017.8226381},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(3).pdf:pdf},
isbn = {9781538630570},
journal = {2017 43rd Latin American Computer Conference, CLEI 2017},
keywords = {Deep neural network,Natural language processing,Sentiment analysis},
pages = {1--5},
title = {{An analysis of convolutional neural networks for sentence classification}},
volume = {2017-Janua},
year = {2017}
}
@article{Radford2018,
abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI).},
author = {Radford, Alec and Salimans, Tim},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Salimans - 2018 - Improving Language Understanding by Generative Pre-Training.pdf:pdf},
journal = {arXiv},
pages = {1--12},
title = {{Improving Language Understanding by Generative Pre-Training}},
url = {https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language{\_}understanding{\_}paper.pdf},
year = {2018}
}
@article{Xie2019,
abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7{\%} with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10{\%} labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
archivePrefix = {arXiv},
arxivId = {1904.12848},
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
eprint = {1904.12848},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(11).pdf:pdf},
pages = {1--19},
title = {{Unsupervised Data Augmentation for Consistency Training}},
url = {http://arxiv.org/abs/1904.12848},
year = {2019}
}
@article{Massie2007,
abstract = {Creating case representations in unsupervised textual case-based reasoning applications is a challenging task because class knowledge is not available to aid selection of discriminatory features or to evaluate alternative system design configurations. Representation is considered as part of the development of a tool, called CAM, which supports an anomaly report processing task for the European Space Agency. Novel feature selection/extraction techniques are created which consider word co-occurrence patterns to calculate similarity between words. These are used together with existing techniques to create 5 different case representations. A new evaluation technique is introduced to compare these representations empirically, without the need for expensive, domain expert analysis. Alignment between the problem and solution space i s measured at a local level and profiles of these local alignments used to evaluate the competence of the system design. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Massie, Stewart and Wiratunga, Nirmalie and Craw, Susan and Donati, Alessandro and Vicari, Emmanuel},
doi = {10.1007/978-3-540-74141-1_25},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Massie et al. - 2007 - From anomaly reports to cases(2).pdf:pdf},
isbn = {9783540741381},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {359--373},
title = {{From anomaly reports to cases}},
volume = {4626 LNAI},
year = {2007}
}
@article{Peng2017a,
abstract = {State-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent deep learning methods remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model (McDepCNN). It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word. Therefore, the model can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN compares favorably to the state-of-the-art rich-feature and single-kernel based methods. In addition, McDepCNN achieves 24.4{\%} relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12{\%} improvement in F1-score over kernel-based methods on "difficult" instances. These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences.},
archivePrefix = {arXiv},
arxivId = {1706.01556},
author = {Peng, Yifan and Lu, Zhiyong},
doi = {10.18653/v1/W17-2304},
eprint = {1706.01556},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng, Lu - 2017 - Deep learning for extracting protein-protein interactions from biomedical literature.pdf:pdf},
number = {Ml},
title = {{Deep learning for extracting protein-protein interactions from biomedical literature}},
url = {http://arxiv.org/abs/1706.01556},
year = {2017}
}
@article{Feng2017,
abstract = {Event detection remains a challenge due to the difficulty at encoding the word se-mantics in various contexts. Previous approaches heavily depend on language-specific knowledge and pre-existing nat-ural language processing (NLP) tools. However, compared to English, not all languages have such resources and tools available. A more promising approach is to automatically learn effective features from data, without relying on language-specific resources. In this paper, we de-velop a hybrid neural network to cap-ture both sequence and chunk information from specific contexts, and use them to train an event detector for multiple lan-guages without any manually encoded fea-tures. Experiments show that our approach can achieve robust, efficient and accurate results for multiple languages (English, Chinese and Spanish).},
annote = {Event detection in three different languages: english, spanish and chinese

proposed a combination of bi-lstm and cnn model for trigger identification and trigger classification in an event

Used ace english and chinese datasets and spanish ere dataset},
author = {Feng, Xiaocheng and Huang, Lifu and Tang, Duyu and Qin, Bing and Ji, Heng and Liu, Ting},
doi = {10.18653/v1/P16-2011},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng et al. - 2017 - A Language-Independent Neural Network for Event Detection.pdf:pdf},
isbn = {9781510827592},
journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
keywords = {event detection,nature language processing,neural networks,representation learning},
number = {September},
pages = {66--71},
title = {{A Language-Independent Neural Network for Event Detection}},
volume = {61},
year = {2017}
}
@article{Warstadt2019,
abstract = {This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.'s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.},
archivePrefix = {arXiv},
arxivId = {1805.12471},
author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
doi = {10.1162/tacl_a_00290},
eprint = {1805.12471},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training(12).pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {625--641},
title = {{Neural Network Acceptability Judgments}},
volume = {7},
year = {2019}
}

@article{fedus2018maskgan,
  title={MaskGAN: better text generation via filling in the\_},
  author={Fedus, William and Goodfellow, Ian and Dai, Andrew M},
  journal={arXiv preprint arXiv:1801.07736},
  year={2018}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{raghunandan2008evaluation,
  title={Evaluation measures for TCBR systems},
  author={Raghunandan, MA and Wiratunga, Nirmalie and Chakraborti, Sutanu and Massie, Stewart and Khemani, Deepak},
  booktitle={European Conference on Case-Based Reasoning},
  pages={444--458},
  year={2008},
  organization={Springer}
}

@inproceedings{10.5555/646268.684025,
author = {Br\"{u}ninghaus, Stefanie and Ashley, Kevin D.},
title = {The Role of Information Extraction for Textual CBR},
year = {2001},
isbn = {3540423583},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 4th International Conference on Case-Based Reasoning: Case-Based Reasoning Research and Development},
pages = {74–89},
numpages = {16},
series = {ICCBR ’01}
}


@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@article{ali2019mfc,
  title={MFC-GAN: class-imbalanced dataset classification using multiple fake class generative adversarial network},
  author={Ali-Gombe, Adamu and Elyan, Eyad},
  journal={Neurocomputing},
  volume={361},
  pages={212--221},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{barbu2019objectnet,
  title={ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author={Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9448--9458},
  year={2019}
}


@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@book{han2011data,
  title={Data mining: concepts and techniques},
  author={Han, Jiawei and Pei, Jian and Kamber, Micheline},
  year={2011},
  publisher={Elsevier}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@techreport{Caragea,
abstract = {Online digital libraries make it easier for researchers to search for scientific information. They have been proven as powerful resources in many data mining, machine learning and information retrieval applications that require high-quality data. The quality of the data highly depends on the accuracy of classifiers that identify the types of documents that are crawled from the Web, e.g., as research papers, slides, books, etc., for appropriate indexing. These classifiers in turn depend on the choice of the feature representation. We propose novel features that result in high-accuracy classifiers for document type classification. Experimental results on several datasets show that our classifiers outperform models that are employed in current systems.},
author = {Caragea, Cornelia and Wu, Jian and {Das Gollapalli}, Sujatha and Giles, C Lee},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caragea et al. - Unknown - Document Type Classification in Online Digital Libraries.pdf:pdf},
keywords = {Emerging Application Case Studies},
title = {{Document Type Classification in Online Digital Libraries}},
url = {http://pdfbox.apache.org/}
}
@techreport{Johnson,
abstract = {This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.},
author = {Johnson, Rie and Zhang, Tong},
file = {:C$\backslash$:/Users/User/Documents/Papers/Deep Learning/5849-semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding.pdf:pdf},
title = {{Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding}}
}
@phdthesis{Massie2006,
annote = {Thesis on TCBR},
author = {Massie, Stewart},
booktitle = {Practice},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Massie - 2006 - Complexity Modelling for Case Knowledge Maintenance in Case-Based Reasoning.pdf:pdf},
number = {December},
title = {{Complexity Modelling for Case Knowledge Maintenance in Case-Based Reasoning}},
year = {2006}
}
@article{Pollock2010,
abstract = {applicability for this approach.},
author = {Pollock, Joy and Waller, Elisabeth and Politt, Rody},
doi = {10.4324/9780203461891_chapter_3},
file = {:C$\backslash$:/Users/User/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollock, Waller, Politt - 2010 - Speech and language processing.pdf:pdf},
journal = {Day-to-Day Dyslexia in the Classroom},
pages = {16--28},
title = {{Speech and language processing}},
year = {2010}
}
